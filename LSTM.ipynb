{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Heart Rhythm Classification\n",
    "\n",
    "information provided: time series (original ECG recordings of different length)\n",
    "\n",
    "to predict: heart rhythm into one of 4 classes\n",
    "\n",
    "Evaluation metric: F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import biosppy.signals.ecg as ecg\n",
    "from biosppy.plotting import plot_ecg\n",
    "import scipy.signal\n",
    "import scipy.stats\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_test_data = pd.read_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x7990</th>\n",
       "      <th>x7991</th>\n",
       "      <th>x7992</th>\n",
       "      <th>x7993</th>\n",
       "      <th>x7994</th>\n",
       "      <th>x7995</th>\n",
       "      <th>x7996</th>\n",
       "      <th>x7997</th>\n",
       "      <th>x7998</th>\n",
       "      <th>x7999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-863.0</td>\n",
       "      <td>-860.0</td>\n",
       "      <td>-857.0</td>\n",
       "      <td>-854.0</td>\n",
       "      <td>-851.0</td>\n",
       "      <td>-849.0</td>\n",
       "      <td>-846.0</td>\n",
       "      <td>-843.0</td>\n",
       "      <td>-838.0</td>\n",
       "      <td>-831.0</td>\n",
       "      <td>...</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-137.0</td>\n",
       "      <td>-132.0</td>\n",
       "      <td>-123.0</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>...</td>\n",
       "      <td>244.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-727.0</td>\n",
       "      <td>-736.0</td>\n",
       "      <td>-748.0</td>\n",
       "      <td>-784.0</td>\n",
       "      <td>-831.0</td>\n",
       "      <td>-874.0</td>\n",
       "      <td>-917.0</td>\n",
       "      <td>-967.0</td>\n",
       "      <td>-1029.0</td>\n",
       "      <td>-1112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>-186.0</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-178.0</td>\n",
       "      <td>-176.0</td>\n",
       "      <td>-174.0</td>\n",
       "      <td>-171.0</td>\n",
       "      <td>-168.0</td>\n",
       "      <td>-164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-103.0</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0     x1     x2     x3     x4     x5     x6     x7      x8      x9  \\\n",
       "0 -863.0 -860.0 -857.0 -854.0 -851.0 -849.0 -846.0 -843.0  -838.0  -831.0   \n",
       "1 -137.0 -132.0 -123.0 -107.0  -79.0  -59.0  -45.0  -47.0   -70.0  -107.0   \n",
       "2 -727.0 -736.0 -748.0 -784.0 -831.0 -874.0 -917.0 -967.0 -1029.0 -1112.0   \n",
       "3 -103.0  -98.0  -92.0  -87.0  -81.0  -77.0  -71.0  -65.0   -61.0   -59.0   \n",
       "4    8.0    2.0   -1.0   -3.0   -5.0   -7.0  -10.0  -12.0   -14.0   -14.0   \n",
       "\n",
       "   ...  x7990  x7991  x7992  x7993  x7994  x7995  x7996  x7997  x7998  x7999  \n",
       "0  ...  231.0  231.0  231.0  231.0  231.0  231.0  231.0  231.0  231.0  231.0  \n",
       "1  ...  244.0  245.0  247.0  250.0  251.0  251.0  247.0  242.0  234.0  225.0  \n",
       "2  ... -188.0 -186.0 -183.0 -180.0 -178.0 -176.0 -174.0 -171.0 -168.0 -164.0  \n",
       "3  ...  -10.0   -2.0   11.0   22.0   32.0   41.0   45.0   48.0   51.0   55.0  \n",
       "4  ...  233.0  233.0  233.0  233.0  233.0  233.0  233.0  233.0  233.0  233.0  \n",
       "\n",
       "[5 rows x 8000 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the id column\n",
    "X_train = X_train.drop(['id'], axis = 1)\n",
    "y_train = y_train.drop(['id'], axis = 1)\n",
    "id_list = X_test_data['id'].values\n",
    "X_test_data = X_test_data.drop(['id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function first truncates the dataframe and selects only the first seq_length columns\n",
    "# then the missing values are filled with the last available value in that row\n",
    "def prepare_data(data: pd.DataFrame, seq_length = 8500):\n",
    "    data = data.iloc[:, :seq_length]\n",
    "    data = data.fillna(method='bfill', axis=1)\n",
    "    data = data.fillna(method='ffill', axis=1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_signals(signals: np.ndarray) -> dict:\n",
    "    bio_signals = []\n",
    "    for i in tnrange(signals.shape[0]):\n",
    "        ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts, heart_rate = ecg.ecg(signal=signals[i], sampling_rate=300, show=False)\n",
    "        bio_signals.append({'ts': ts, 'filtered': filtered, 'rpeaks': rpeaks, 'templates_ts': templates_ts, 'templates': templates, 'heart_rate_ts': heart_rate_ts, 'heart_rate': heart_rate})\n",
    "    return bio_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_peaks(signals, prominence = 300):\n",
    "    peaks = np.asarray([scipy.signal.find_peaks(signals[i], prominence=prominence)[0].shape[0] for i in range(signals.shape[0])])\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_statistics(bio_signals):\n",
    "    T = 1.0/300.0\n",
    "    # signal statistics\n",
    "    signal_stats = np.zeros((len(bio_signals), 6))\n",
    "    for i in range(signal_stats.shape[0]):\n",
    "        num_peaks = find_num_peaks(bio_signals[i]['templates'], prominence = 30)\n",
    "        signal_stats[i, 0] = bio_signals[i]['filtered'].mean()\n",
    "        signal_stats[i, 1] = np.var(bio_signals[i]['filtered'])\n",
    "        signal_stats[i, 2] = scipy.stats.skew(bio_signals[i]['filtered'])\n",
    "        signal_stats[i, 3] = scipy.stats.kurtosis(bio_signals[i]['filtered'])\n",
    "        signal_stats[i, 4] = num_peaks.mean()\n",
    "        signal_stats[i, 5] = num_peaks.max()\n",
    "        #signal_stats[i, 2] = np.max(bio_signals[i]['filtered']) - np.min(bio_signals[i]['filtered'])\n",
    "\n",
    "    # heart rate statistics\n",
    "    hr_stats = np.zeros((len(bio_signals), 4))\n",
    "    for i in range(hr_stats.shape[0]):\n",
    "        if bio_signals[i]['heart_rate'].shape[0] == 0:\n",
    "            continue\n",
    "        hr_stats[i, 0] = bio_signals[i]['heart_rate'].mean()\n",
    "        hr_stats[i, 1] = np.var(bio_signals[i]['heart_rate'])\n",
    "        hr_stats[i, 2] = np.amax(bio_signals[i]['heart_rate'])\n",
    "        hr_stats[i, 3] = np.amin(bio_signals[i]['heart_rate'])\n",
    "    # rpeak statistics\n",
    "    rr_int_stats = np.zeros((len(bio_signals), 11))\n",
    "    for i in range(hr_stats.shape[0]):\n",
    "        if bio_signals[i]['rpeaks'].shape[0] == 0:\n",
    "            continue\n",
    "        rr_int = np.diff(T*1000*bio_signals[i]['rpeaks'])\n",
    "        rr_int_stats[i, 0] = rr_int.mean()\n",
    "        rr_int_stats[i, 1] = np.var(rr_int)\n",
    "        rr_int_stats[i, 2] = np.amin(rr_int)#scipy.stats.kurtosis(rr_int)\n",
    "        rr_int_stats[i, 3] = np.amax(rr_int)#scipy.stats.skew(rr_int)\n",
    "        rr_int_diff = np.abs(np.diff(rr_int))\n",
    "        rr_int_stats[i, 4] = rr_int_diff.mean()\n",
    "        rr_int_stats[i, 5] = np.var(rr_int_diff)\n",
    "        rr_int_stats[i, 6] = np.amin(rr_int_diff) #scipy.stats.kurtosis(rr_int_diff)\n",
    "        rr_int_stats[i, 7] = np.amax(rr_int_diff) #scipy.stats.skew(rr_int)\n",
    "        rr_int_diff_sq = np.square(rr_int_diff)\n",
    "        #rr_int_stats[i, 8] = rr_int_diff_sq.mean()\n",
    "        #rr_int_stats[i, 9] = np.std(rr_int_diff_sq)\n",
    "        #rr_int_stats[i, 10] = scipy.stats.kurtosis(rr_int_diff_sq)\n",
    "        #rr_int_stats[i, 11] = scipy.stats.skew(rr_int)\n",
    "        rr_int_stats[i, 8] = np.sqrt(np.mean(rr_int_diff_sq))\n",
    "        # pnn 20/50\n",
    "        rr_int_stats[i, 9] = np.sum(rr_int_diff > 20).astype(np.float32)/rr_int_diff.shape[0]\n",
    "        rr_int_stats[i, 10] = np.sum(rr_int_diff > 50).astype(np.float32)/rr_int_diff.shape[0]\n",
    "\n",
    "    # dft\n",
    "    start = 15\n",
    "    end = 20\n",
    "    fourier = np.zeros((len(bio_signals), start + end))\n",
    "    mean_qrs = np.zeros((len(bio_signals), 1))\n",
    "    for i in range(len(bio_signals)):\n",
    "        signal = bio_signals[i]['filtered']\n",
    "        rpeaks = bio_signals[i]['rpeaks']\n",
    "        qrs = np.asarray([signal[rpeaks[j] - start : rpeaks[j] + end] for j in range(rpeaks.shape[0])])\n",
    "        f_t = np.fft.fft(qrs, axis=1)\n",
    "        f_t = np.absolute(f_t)\n",
    "        f_t = f_t.mean(axis=0)\n",
    "        fourier[i] = f_t\n",
    "        mean_qrs[i, 0] = qrs.mean(axis=0).mean()\n",
    "    return np.concatenate((signal_stats, hr_stats, rr_int_stats, fourier, mean_qrs), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training data\n",
    "1. Truncating; Imputing missing values with the last appearing value in the Time Series\n",
    "2. Extracting bio signals from the prepared training dataset\n",
    "3. Manually extracting statitsics from the bio signals\n",
    "4. Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d73701029954ee998cfd28729a8ebdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = prepare_data(X_train, seq_length = 8000)\n",
    "bio_signals = get_bio_signals(X_train.values)\n",
    "X_train_extracted = extract_statistics(bio_signals)\n",
    "scaled = StandardScaler().fit_transform(X_train_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different Classifiers (Best so far: XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "classifier = CatBoostClassifier(iterations=2,\n",
    "                           depth=2,\n",
    "                           learning_rate=1,\n",
    "                           loss_function='MultiClass',\n",
    "                           eval_metric='TotalF1',\n",
    "                           random_seed=42,\n",
    "                           l2_leaf_reg=3,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "classifier = LGBMClassifier(colsample_bytree=0.9,   \n",
    "                           objective='multi:softmax',\n",
    "                           boosting_type='dart',\n",
    "                           gamma=1,  \n",
    "                           num_leaves=62,\n",
    "                           learning_rate=0.2,\n",
    "                           max_depth=6,       \n",
    "                           min_child_weight=1,   \n",
    "                           n_estimators=1000,                                                     \n",
    "                           reg_alpha=3,       \n",
    "                           reg_lambda=0.5,       \n",
    "                           subsample=0.9,         \n",
    "                           seed=42\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = XGBClassifier(colsample_bytree=0.9,   \n",
    "                           objective='multi:softmax',\n",
    "                           gamma=0.5,  \n",
    "                           num_class=4,\n",
    "                           learning_rate=0.2,\n",
    "                           max_depth=6,       \n",
    "                           min_child_weight=1,   \n",
    "                           n_estimators=200,                                                     \n",
    "                           reg_alpha=1.1,       \n",
    "                           reg_lambda=0.45,       \n",
    "                           subsample=0.6,         \n",
    "                           seed=42\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "nb_filters = 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5117, 57)\n",
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asana\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:30: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_9_input to have 3 dimensions, but got array with shape (3428, 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-7d16c42c3f58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m model.fit(train_X, train_y, batch_size=batch_size,\n\u001b[1;32m---> 30\u001b[1;33m           nb_epoch=nb_epoch, validation_data=(test_X, test_y))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv1d_9_input to have 3 dimensions, but got array with shape (3428, 57)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(X_train_extracted.shape)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_train_extracted, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "nrows, ncols = train_X.shape\n",
    "train_X.reshape(nrows, ncols, 1)\n",
    "\n",
    "nrows, ncols = test_X.shape\n",
    "test_X.reshape(nrows, ncols, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(nb_filters, 3, input_shape=train_X.shape, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(nb_filters, 3, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"ok\")\n",
    "batch_size=10\n",
    "nb_epoch=10\n",
    "model.fit(train_X, train_y, batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kfold CV and reporting f scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f37c448e69e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "y_train = y_train['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "classifier = CatBoostClassifier(iterations=4,\n",
    "                                classes_count=4,\n",
    "                                od_pval=1e-3,\n",
    "                                depth=7,\n",
    "                                learning_rate=1,\n",
    "                                loss_function='MultiClass',\n",
    "                                eval_metric='TotalF1',\n",
    "                                random_seed=42,\n",
    "                                l2_leaf_reg=10,\n",
    "                                verbose=True)\n",
    "\n",
    "classifier = LGBMClassifier(colsample_bytree=0.9,   \n",
    "                            objective='multi:softmax',\n",
    "                            gamma=1,  \n",
    "                            num_leaves=62,\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=10,       \n",
    "                            min_child_weight=1,   \n",
    "                            n_estimators=1000,                                                     \n",
    "                            reg_alpha=3,       \n",
    "                            reg_lambda=0.5,       \n",
    "                            subsample=0.9,         \n",
    "                            seed=42\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asana\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8047661595796678, 0.008489192449336722)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scores = []\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "for train, test in kfold.split(X_train_extracted, y_train):\n",
    "    train_X, eval_X, train_Y, eval_Y = X_train_extracted[train], X_train_extracted[test], y_train[train], y_train[test]\n",
    "    scaler = StandardScaler().fit(train_X)\n",
    "    train_X = scaler.transform(train_X)\n",
    "    eval_X = scaler.transform(eval_X)\n",
    "    \n",
    "    classifier.fit(train_X, train_Y)\n",
    "    scores.append(f1_score(eval_Y, classifier.predict(eval_X), average='micro'))\n",
    "    \n",
    "scores = np.asarray(scores)\n",
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the most relevant bio-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 57 artists>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "mut_info_res = mutual_info_classif(X_train_extracted, y_train['y'].values, discrete_features=False, random_state=42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(list(range(len(mut_info_res))), mut_info_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# this is the Grid of hyperparameters to search over\n",
    "hyperparameter_grid={\n",
    "    'objective': ['multi:softmax'],\n",
    "    'gamma': [0.5, 1, 2, 4],\n",
    "    'max_depth': [6, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'min_child_weight': [1, 3, 4, 6],\n",
    "    'colsample_bytree': [0.6, 0.8, 0.9],\n",
    "    'num_class': [4],\n",
    "    'random_state': [42],\n",
    "    'n_estimators': [150, 200, 250],\n",
    "    'min_child_weight': [1, 2, 3, 4] \n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score , average='micro')\n",
    "\n",
    "# this returns the best model\n",
    "search = GridSearchCV(estimator=XGBClassifier(),    \n",
    "                            param_grid=hyperparameter_grid,\n",
    "                            cv=StratifiedKFold(n_splits=5, random_state=42),\n",
    "                            scoring=scorer, #use f1 score\n",
    "                            n_jobs=-1\n",
    "                        )\n",
    "\n",
    "\n",
    "search.fit(scaled, y_train['y'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.9, gamma=1, learning_rate=0.01, max_depth=10,\n",
       "               min_child_weight=1, n_estimators=1000, num_leaves=62,\n",
       "               objective='multi:softmax', reg_alpha=3, reg_lambda=0.5, seed=42,\n",
       "               subsample=0.9)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifier.fit(scaled, y_train['y'].values)\n",
    "classifier.fit(scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on test data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db12d9ff0d1c4f658c64eabea96e5948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3411), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = prepare_data(X_test_data, seq_length = 8000)\n",
    "bio_signals_test = get_bio_signals(X_test.values)\n",
    "X_test_extracted = extract_statistics(bio_signals_test)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_submission = classifier.predict(X_test_scaled)\n",
    "sample_pd = pd.DataFrame({'id': id_list, 'y': y_pred_submission})\n",
    "sample_pd.to_csv(r'sample_final_task3.csv', index = False)  \n",
    "print(\"The csv file has been created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Best result without feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if we perform feature selection based on mutual info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The csv file has been created successfully\n"
     ]
    }
   ],
   "source": [
    "mut_info_threshold = 0.025 # got worse results with selecting features\n",
    "\n",
    "X_train_selected = X_train_extracted[:, mut_info_res > mut_info_threshold]\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train_selected)\n",
    "\n",
    "classifier.fit(X_train_scaled, y_train['y'].values)\n",
    "\n",
    "X_test_selected = X_test_extracted[:, mut_info_res > mut_info_threshold]\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test_selected)\n",
    "\n",
    "\n",
    "y_pred_submission = classifier.predict(X_test_scaled)\n",
    "sample_pd = pd.DataFrame({'id': id_list, 'y': y_pred_submission})\n",
    "sample_pd.to_csv(r'sample_final_task3.csv', index = False)  \n",
    "print(\"The csv file has been created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving them as csvs\n",
    "X_test.to_csv(r'X_test_prepared.csv', index = False)  \n",
    "pd.DataFrame(X_test_extracted).to_csv(r'X_test_extracted.csv', index = False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
